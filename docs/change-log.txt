Changes in 0.8.2:

When Kontroller was introduced a new package in KaBoom was created to integrate with the Kafka/KaBoom clusters via an API.  This release of KaBoom is a significant effort to refactor the entire code base to use those APIs internally.  The complex data structures, arrays and hash maps that exposed the cluster details are replaced  by an intuitive object oriented API.  

There's also two new features that Matt requested after discovering lease conflicts between boom files.  Previously, when a new leader took over it would wait 30 seconds, then ran the configured load balancer.  It was possible during a rolling restart for there to be clients not yet registered when the balancer ran.  This could lead to uneven work distribution.  This imbalance would then be resolved 10 minutes later when  the balancer ran again.   An even nastier scenario where these short-lived assignments could be handed out twice was possible if the newly elected leader was a node that hadn't yet been restarted.  Depending on how quickly workers realized they were no longer assigned the partition and how quickly another worker tried to create the same boom file, the possibility existed that two workers could be trying to manage boom files in HDFS with the same path.

To resolve the balancer running to early, there's now a new running configuration option 'New Leader Calm Down Delay (ms)' defaulted to 30 seconds.  For busy clusters like at BlackBerry, this should be set to a high enough time to account for cluster wide restarts, 1-2 minutes.

To resolve the possibility that two workers on separate clients could have overlapping assignments a new abstract class AsynchronousAssignee has been introduced that requires a thread to provide a ZK assignment path and an identity (byte[]) expected to be found in the assignment.  It then uses an InterProcessMutex to lock the assignment.   To further safeguard against potential problems a watch is placed on the zookeeper connection and the pause() method is called if the connection becomes suspended.  If the connection is restored it checks that it's still assigned and resumes the worker.  Another watch is placed on the znode for the assignment and the worker is scheduled to stop() the moment it's no longer the assignee.

A note on shutdown/shutdown hooks:  Sometime since the switch to log4j2 KaBoom's shutdown hook was being overwritten by log4j2's shutdown hook.  This prevented worker shifts from finishing properly and could result in metadata not being persisted during shutdown.  To prevent this you can disable the log4j2 shutdown hook in your log4j2 configuration's opening configuration tag with <configuration ... shutdownHook="disable">

Complete change log below...

- Fixes some missing/improperly formatted license headers
- The leader now writes their client ID to ZK
- Sprints that never receive a message previously (0.8.0, 0.8.1) didn't update ZK with a offset timestamp, now they will store the sprint end time 
- New startup configuration variable zkRootPathKaBoom configured via property: kaboom.zk.root.path (default: "/kaboom")
- New startup configuration variable zkRootPathKafka configured via property: kafka.zk.root.path (default: "/")
- New startup configuration variable zkRootPathClients configured via property: kaboom.zk.root.path.clients (default: zkRootPath + "/clients")
- New startup configuration variable zkRootPathTopicConfigs configured via property: kaboom.zk.root.path.topic.configs (default: zkRootPath + "/topics")
- New startup configuration variable zkRootPathPartitionAssignments configured via property: kaboom.zk.root.path.partition.assignments (default: zkRootPath + "/assignments")
- New startup configuration variable zkRootPathFlagAssignments configured via property: kaboom.zk.root.path.flag.assignments (default: zkRootPath + "/flag-assignments")
- New startup configuration variable zkPathRunningConfig configured via property: kaboom.zk.path.runningConfig (default: zkRootPath + "/config")
- New startup configuration variable zkPathLeaderClientId configured via property: kaboom.zk.leader.clientId(default: zkRootPath + "/leader")
- Refactored any zkPaths to refer to the paths above in StartupConfig
- Removed StartupConfig.getTopicConfig(topicName)
- Removed a slew of unneccessary getters from StartupConfig
- ReadyFlagWriter now uses KaBoomTopic from the api package instead of StateUtils to get topics and their partition lists
- Added public KaBoomTopicConfig getConfig() to KaBoomTopic in the API package making KaBoomTopic more authoritative
- Added new class KaBoomPartitionDetails to replace the private inner class KaBoomTopic.PartitionDetails that was being exposed via the getter
- Removed the ZK traversal public method to get the oldest partition timestamp for a topic name from ReadyFlagWriter
- Added public long KaBoomTopic.oldestPartitionOffset() to replace above
- Removed usage of the Kafka API in ReadyFlagWriter as above changes in KaBoomTopic now include all the info required
- Removed StateUtils (now API usage is how  metadata and configurations are discovered)
- Removed KaBoomNodeInfo in favor of the API KaBoomClient
- Added KaBoomClient.calculateTargetLoad(int totalPartitions, int totalWeight)
- KaBoomClient now extends ZkVersioned 
- Migrated KaBoom's ephemeral node for client to be a persisted ephemeral mode KaBoomClient removing the yaml dependencies
- KaBoom client ephemeral node serialization is now updated as the client's attributes change
- Supports deleting topic configuration through the API
- Supports reloading topic configuration through the API and workers restart gracefully
- Added the concept of graceful shutdown of the workers
- Added two new metrics for the number of workers found dead and the number of workers gracefully restarted
- Improved how the leader removes assignments from disconnected clients
- Improved how the ReadyFlagController balances flag propagator paths
- New running configuration option newLeaderCalmDownDelay (milliseconds, default 30 * 1000) The amount of time to wait after a new leader is elected before it starts leading
- Prefixes the KaBoom Client ID to all boom files to ensure that race conditions over partition re-assignments don't lead to lease conflicts or missing leases
- Moved the method that gets the current offset from ZK into the Worker's private class
- Renamed private class WorkerSprint in Worker to WorkerShift
- Added a bunch of convienience methods in WorkerShift isOver() isFinsihed() isTimeToFinish()
- Added a new abstract class AsynchronousAssignee for threads that are assigned work from ZK
- Reduced the verbosity of the error messages when the KaBoomTopic are missing Kafka topics or are not configured
- Added a new metric "kaboom:topic:<topic>:timestamp parse errors" for tracking and monitoring timestamp parsing errors
- Added a new metric "kaboom:topic:<topic>:PRI parse errors" for tracking and monitoring PRI parsing errors
- Removed the error log that a timestamp could not be parsed
- Removed the error log that a PRI could not be parsed

Changes in 0.8.1:

- New metric, kaboom:total:gracefully restarted workers

Changes in 0.8.0:

- Refactored configuration into startup/running, running migrated to ZK
- Introduction of worker sprints
- Intended to be manage via Kontroller API and web interface

Changes in 0.7.16-HF1

- Improves exception handling and reduce file corruption when closing boom files to ensure that any problems closing the file result in the worker aborting the file.

Changes in 0.7.16

- Adds a new stand-alone utility to write a boom file for a specific partition, start offset and end offset to a specific destination
- Adds a new meter metric: kaboom:total:dead workers

Changes in 0.7.15 HF1

- Fixes IPGBD-3830 [Kaboom] Bug when handling zero padded date field

Changes in 0.7.15

- Ensures that only topics with unique HDFS root paths are examined
- New configuration option: kaboom.propagate.ready.flags.delay.ms (long, how often wait between paths, default 0) can be used to ease the burden on the name nodes if there's a massive amount of HDFS path traversal required 

Changes in 0.7.14

- Moved the READY flag propagation to KaBoom (from an older to-be-deprecated project
- New JMX metric: kaboom:topic:<topicName>:flag propagator timer (timer for recursive HDFS directory traversal)
- New JMX metric: kaboom:topic:<topicName>:flag propagator paths checked (merter for number of paths checked)
- New JMX metric: kaboom:topic:<topicName>:flags written (merter for number of flags created)
- New configuration option: kaboom.propagate.ready.flags (boolean, default false)
- New configuration option: kaboom.propagate.ready.flags.frequency (long, how often in ms to spawn propagator thread, default 10 * 60 * 1000)
>>>>>>> ba5a01b... changelog

- Supports bdp-common 0,0.6 which provides all logging and monitoring deps
- Instruments log4j2 with io.dropwizard.metrics
- Adds a new API 
=======
Changes in 0.7.13

- Added kafkaOffset argument to FastBoomWriter.writeLine()
- Added a lastKafkaOffset to FastBoomWriter
- FastBoomWriter now keeps track of lastKafkaOffset and lastMessageTimestamp
- Changed the periodicCloseExpiredPoll in TimeBasedHdfsOutputPath to store the expired boom file's lastKafakaOffset and lastMessageTimestamp instead of the worker's current values
- Added a new global configuration property max.open.boom.files.per.partition (default: 5)  to limit the number of open boom files per partition
- Modified TimeBasedHdfsOutputPath to close off the oldest FastBoomWriter when the number of open boom files is greater than max.open.boom.files.per.partition, and update zookeeper with lastKafakaOffset and lastMessageTimestamp

Changes in 0.7.12 HF4

- ReadFlagWriter now doesn't check for the existence of the data directory when it's writing flags

Changes in 0.7.12. HF3

- New configuration option boom.file.expired.grace.time.ms (default: 30 * 1000, thirty seconds): The time after a TimeBasedHdfsOutputPath output file expires before it's closed via the periodic file close interval triggered upon each message consumed from Kafka.  This option is introduced to make the previously hard coded value configurable and uses the same default value as the previous hard coded value.
- New configuration option forced.zk.offsetTimestamp.update.time.ms (default: 10 * 60 * 1000, ten minutes): Ensure that very quiet partitions are updating their offset timestamp in ZK even when they are not receiving any messages.  If the last received message was during the previous hour and it's been more than this amount of  milliseconds then write the start of the hour's timestamp into ZK for the partition (providing it hasn't already been stored for the current hour already).
- New configuration option kaboom.server.sleep.duration.ms (default: 10 000, ten seconds): Exposes a configuration option for a previously hard coded property value with the same default value.  This is the time waited after a worker processes it's work assignments before it fetches another update from ZK and processes again.
- Fixes the NullPointerException caused by improper handling of the NoNodeException that very rarely occurs when a worker fetches it's assignments.  The null assignee is now logged and ignored and skipped until the next round of worker assignments is reviewed (after the newly introduced: kaboom.server.sleep.duration.ms).
- Fixes the "hung KaBoom workers" by sending a basic ping() health check to each KaBoom worker.  The worker has until kaboom.server.sleep.duration.ms to respond with a pong before the KaBoom server will send it a kill request and send an interrupt to the worker's thread.  The worker (if it comes back to acknowledge it's been killed) will throw an exception that will get caught and trigger the abort sequence (delete files, and stop).

Changes in 0.7.12:

- Fixes bug that threw an NPE when there was no work assigned to a client and the load balancer tries to check if it's over worked

Changes in 0.7.11:

- Adds an additional kafka ready flag in the topic root
- Logs a warning if the load balancer is still running after leader.sleep.duration.ms

Changes in 0.7.10:

- Adds a new optional configuration option (String) kaboom.kafkaReady.flag.filename, default=_KAFKA_READY
>>>>>>> origin/kaboom-0.7.13

Changes in 0.7.9:

- Fixes a ReadyFlagWriter bug that would write flags to the current hour
- Adds logging around the maxTimestamps that are stored in ZK
- Depends on common-utils-dev 0.0.5 (to get the new ZK get/set utility)

Changes in 0.7.8:

- Adds a total compression ratio histogram

Changes in 0.7.7:

- Adds new optional configuration option (Short): kaboom.deflate.compression.level, default=6
- Adds new optional configuration option (Short): topic.<topic>.compression.level
- Adds new histogram metric for compression ratio 

Changes in 0.7.6:

- Resolves hostname bug in the LocalLoadBalancer

Changes in 0.7.5

- Improved ReadyFlagWriter logic: Previous versions were buggy and had too many operations included in their hourly loop instead of in the topic loop
- Removes checks on _READY flag in ReadyFlagWriter (_READY flags from LogDriver's LogMaintenance are deprecated by KaBoom 0.7.1 and later)
- Fixes bugs related to concurrent access on shared TimeBasedHdfsOutputPath objects (each worker now instantiates their own)
- Reduced INFO level log messages throughout to limit logs to more important messages
- Separates the pre-install script for DEB and RPM (as DEB's don't require the /var/run/kaboom)
- Adds new optional configuration option (Long): leader.sleep.duration.ms, default=10 * 60 * 1000
- Adds new optional configuration option (String) kaboom.load.balancer.type, default=even


Changes in 0.7.4:

- Abstracts load balancing and adds two implementation: even and local

Changes in 0.7.3:

- Adds native compression 

Changes in 0.7.2:

- Bumps Krackle dependency to 0.7.10 to have the consumer's broker socket have the keep alive flag set

Changes in 0.7.1:

- New timer metrics for HDFS flush time for topic-partition, topic, and total per server
- New meter metrics for boom writes for topic-partition, topic, and total per server
- Adds new required configuration option: hadoop.fs.uri
- Adds new required topic configuration for HDFS root directory (string): topic.<topicName>.hdfsRootDir
- Supports multiple numbered template based HDFS output paths per topic 
- Topic HDFS output paths are now configurable to be left open for specific durations
- Adds new optional configuration option (boolean): kaboom.useTempOpenFileDirectory
- Adds new optional configuration option (Integer): boom.file.buffer.size, default=16384
- Adds new optional configuration option (Short): boom.file.replicas, default=3
- Adds new optional configuration option (Long): boom.file.block.size=268435456
- Adds new optional configuration option (String): boom.file.temp.prefix, default=_tmp_
- Adds new optional configuration option (Long): boom.file.flush.interval, default=30000
- Adds new optional configuration option (Long): boom.file.close.expired.interval, default=60000


Changes in 0.7.0:

- Deprecates all CDH-specific content, configuration, and project files
- New dependency on Krackle 0.7.7 for configuring socket timeouts
- New KaboomConfiguration class that encapsulates all the configuration
- New accessor methods for instantiating CuratorFramework objects
- Project builds produce an RPM artifact 
- Fixes synchronization on non-final workersLock object (used when instantiating metrics)
- Removes unused imports
- Worker.java, int length; byte version; int pos; are no longer initialized with default values that are never used
- New method: private Map<String, String> getTopicPathsFromProps(Properties props)
- New method: private Map<String, String> getTopicProxyUsersFromProps(Properties props)
- new method: private Properties getProperties()


Changes in 0.6.10:

- Re-formats source for Kaboom and Worker class 
- Adds offset overrides feature for single partitions to be set to specific offsets in ZK
- Adds  feature and configuration property to sink to lower offsets when offsets surpass the high watermark
- Re-writes the offset handling code for when last offsets do not match expected offset
- Adds new dependency to the new com.blackberry.common.props that simplifies parsing property files and will eventually be enhanced with ZK support
	
